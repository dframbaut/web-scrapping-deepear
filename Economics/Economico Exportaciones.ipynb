{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import boto3\n",
    "from urllib.parse import unquote\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from supabase import create_client, Client\n",
    "import warnings\n",
    "import os \n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "links_exportaciones = []\n",
    "# URL de la página que deseas capturar\n",
    "url = \"https://www.one.gob.do/datos-y-estadisticas/\"\n",
    "\n",
    "# Realiza la solicitud HTTP\n",
    "response = requests.get(url)\n",
    "response.encoding = 'utf-8'\n",
    "\n",
    "# Analiza el contenido HTML\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Encuentra todos los elementos 'div' con la clase 'elem'\n",
    "bases = soup.find_all('div', class_='elem')\n",
    "\n",
    "for base in bases:\n",
    "    # Encuentra el div con la clase 'collapse' y el id especificado\n",
    "    collapse_div = base.find('div', class_='collapse', id='acc_44c50024-924d-46d5-97f4-40d726159875')\n",
    "   \n",
    "    if collapse_div:\n",
    "        # Encuentra todos los enlaces 'a' dentro de ese div\n",
    "        links = collapse_div.find_all('a')\n",
    "       \n",
    "        # Imprime todos los href de los enlaces encontrados\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            if href and (href.endswith('.xlsx') or href.endswith('.xls')):\n",
    "                print(href)\n",
    "                links_exportaciones.append(href)\n",
    "\n",
    "# Encabezados HTTP personalizados\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Columnas que queremos extraer\n",
    "columnas = ['Transacción','Año','Mes','Cod_Seccion', 'Desc_Seccion', 'Cod_Capitulo', 'Desc_Capitulo', 'Cod_Partida', 'Desc_Partida', 'Cod_Arancel', 'Desc_Arancel', 'Region', 'Pais_Destino', 'Via_Transporte', 'Regimen_Aduanero','Colecturia_ID', 'Colecturia', 'Peso', 'V_FOB_USD']\n",
    "#'V_CIF_USD'\n",
    "# Lista para almacenar los DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Lista para registrar errores\n",
    "errores = []\n",
    "\n",
    "# Descargar y procesar los archivos Excel\n",
    "for link in links_exportaciones:\n",
    "    for intento in range(3):  # Intentar hasta 3 veces\n",
    "        try:\n",
    "            print(f\"Procesando: {link}\")\n",
    "            response = requests.get(link, headers=headers, timeout=30)\n",
    "            response.raise_for_status()  # Verificar si hubo algún error en la solicitud\n",
    "\n",
    "            # Leer el contenido del archivo Excel\n",
    "            df = pd.read_excel(BytesIO(response.content), usecols=columnas)\n",
    "            print(f\"Archivo leído exitosamente: {link}\")\n",
    "            dataframes.append(df)\n",
    "            break  # Salir del bucle si el archivo se descargó correctamente\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error al procesar {link}, intento {intento + 1}: {e}\")\n",
    "            time.sleep(5)  # Esperar 5 segundos antes de reintentar\n",
    "            if intento == 2:  # Si es el último intento, registrar el error\n",
    "                errores.append(link)\n",
    "        except Exception as e:\n",
    "            print(f\"Error inesperado al procesar {link}: {e}\")\n",
    "            errores.append(link)\n",
    "            break\n",
    "\n",
    "# Consolidar todos los DataFrames en uno solo\n",
    "df_final = pd.concat(dataframes, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una copia del DataFrame original\n",
    "working_df = df_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURACIÓN INICIAL\n",
    "# =============================================================================\n",
    "# Este bloque establece columnas iniciales con valores estáticos y ordena el DataFrame.\n",
    "# Se asume que 'working_df' está definido anteriormente.\n",
    "\n",
    "# Añadir columnas iniciales con valores estáticos\n",
    "working_df['Código ISO'] = 'DOM'\n",
    "working_df['País'] = 'República Dominicana'\n",
    "working_df['Departamento'] = 'Nacional'\n",
    "working_df['Ciudad'] = 'Nacional'\n",
    "working_df['Categoría'] = 'Sector Externo'\n",
    "working_df['Sub-Categoría'] = 'Exportaciones'\n",
    "\n",
    "# Ordenar las columnas en el DataFrame\n",
    "column_order = [\n",
    "    'Código ISO',  'País', 'Departamento', 'Ciudad', 'Categoría', 'Sub-Categoría',\n",
    "    'Pais_Destino', 'Region', 'Via_Transporte', 'Colecturia', 'Regimen_Aduanero',\n",
    "    'Cod_Seccion', 'Cod_Capitulo', 'Cod_Partida', 'Cod_Arancel',\n",
    "    'Transacción', 'Año', 'Mes',   'Desc_Seccion', 'Desc_Capitulo',\n",
    "    'Desc_Partida','Desc_Arancel', 'Peso', 'V_FOB_USD',\n",
    "    'Colecturia_ID']\n",
    "working_df = working_df[column_order]\n",
    "\n",
    "# =============================================================================\n",
    "# PREPROCESAMIENTO DE DATOS\n",
    "# =============================================================================\n",
    "# Se eliminan guiones \"-\" en las columnas descriptivas y se generan métricas base.\n",
    "# Además, se crea una columna 'Fecha' a partir de Año/Mes y se elimina estas columnas.\n",
    "\n",
    "# Preprocesar texto: reemplazar \"-\" por espacios y eliminar espacios extra\n",
    "for col in ['Desc_Seccion', 'Desc_Capitulo', 'Desc_Partida', 'Desc_Arancel']:\n",
    "    working_df[col] = working_df[col].str.replace('-', ' ').str.strip()\n",
    "\n",
    "# Crear columnas derivadas a partir de las columnas base\n",
    "working_df['Toneladas'] = working_df['Peso'] / 1000.0\n",
    "working_df['Millones_USD_FOB'] = working_df['V_FOB_USD'] / 1_000_000\n",
    "working_df['Millones_Toneladas_metricas'] = working_df['Toneladas'] / 1_000_000\n",
    "\n",
    "# Crear columna 'Fecha' a partir de 'Año' y 'Mes' y luego eliminar dichas columnas\n",
    "working_df['Fecha'] = pd.to_datetime(\n",
    "    working_df['Año'].astype(str) + '-' + working_df['Mes'].apply(lambda x: f'{x:02}') + '-01'\n",
    ")\n",
    "working_df.drop(['Año', 'Mes'], axis=1, inplace=True)\n",
    "\n",
    "# Ordenar el DataFrame por criterios lógicos (fecha y desagregaciones)\n",
    "working_df.sort_values(by=['Fecha', 'Desc_Seccion', 'Desc_Capitulo', 'Desc_Partida', 'Desc_Arancel'],\n",
    "                       ascending=True, inplace=True)\n",
    "\n",
    "# =============================================================================\n",
    "# AGRUPACIÓN MENSUAL Y CÁLCULOS DE SUMAS A 12 MESES\n",
    "# =============================================================================\n",
    "# Se agrupan los datos mensuales por desagregaciones y se obtienen sumas.\n",
    "# Luego se calculan sumas móviles a 12 meses (12M) para las métricas clave.\n",
    "\n",
    "# Agrupar mensualmente, agregando columnas con función 'first' o 'sum' según corresponda\n",
    "agrupadas = working_df.groupby(\n",
    "    ['Fecha', 'Desc_Seccion', 'Desc_Capitulo', 'Desc_Partida', 'Desc_Arancel',\n",
    "     'Cod_Seccion', 'Cod_Capitulo', 'Cod_Partida', 'Cod_Arancel'],\n",
    "    as_index=False\n",
    ").agg({\n",
    "    'Toneladas': 'sum',\n",
    "    'Millones_USD_FOB': 'sum',\n",
    "    'Millones_Toneladas_metricas': 'sum',\n",
    "    'Pais_Destino': 'first',\n",
    "    'Region': 'first',\n",
    "    'Via_Transporte': 'first',\n",
    "    'Colecturia_ID': 'first',\n",
    "    'Colecturia': 'first',\n",
    "    'Regimen_Aduanero': 'first'\n",
    "})\n",
    "\n",
    "# Extraer el mes para cálculos de rolling y variaciones\n",
    "agrupadas['Mes'] = agrupadas['Fecha'].dt.month\n",
    "\n",
    "# Calcular sumas móviles a 12 meses\n",
    "grouped = agrupadas.groupby(['Desc_Seccion', 'Desc_Capitulo', 'Desc_Partida'])\n",
    "agrupadas['Toneladas_12M'] = grouped['Toneladas'].transform(lambda x: x.rolling(window=12, min_periods=1).sum())\n",
    "agrupadas['Millones_USD_FOB_12M'] = grouped['Millones_USD_FOB'].transform(lambda x: x.rolling(window=12, min_periods=1).sum())\n",
    "agrupadas['Millones_Toneladas_metricas_12M'] = grouped['Millones_Toneladas_metricas'].transform(lambda x: x.rolling(window=12, min_periods=1).sum())\n",
    "\n",
    "# =============================================================================\n",
    "# CÁLCULO DE VALOR ANTERIOR Y VARIACIONES ANUALES\n",
    "# =============================================================================\n",
    "# Se calcula el valor del mismo mes del año anterior para cada métrica y\n",
    "# a partir de allí se obtiene la variación porcentual anual.\n",
    "\n",
    "columnas_id = [\n",
    "    'Fecha', 'Desc_Seccion', 'Desc_Capitulo', 'Desc_Partida', 'Desc_Arancel',\n",
    "    'Cod_Seccion', 'Cod_Capitulo', 'Cod_Partida', 'Cod_Arancel', 'Mes',\n",
    "    'Pais_Destino', 'Region', 'Via_Transporte', 'Colecturia_ID', 'Colecturia', 'Regimen_Aduanero'\n",
    "]\n",
    "\n",
    "columnas_con_valor = [c for c in agrupadas.columns if c not in columnas_id]\n",
    "\n",
    "# Calcular valor anterior (mismo mes del año previo) y variaciones\n",
    "for col in columnas_con_valor:\n",
    "    agrupadas[f'{col}_valor_anterior'] = agrupadas.groupby('Mes')[col].shift(12)\n",
    "    agrupadas[f'variacion {col}'] = ((agrupadas[col] - agrupadas[f'{col}_valor_anterior']) /\n",
    "                                     agrupadas[f'{col}_valor_anterior'] * 100)\n",
    "\n",
    "# Filtrar columnas para el melt (excluyendo las de valor_anterior)\n",
    "final_columnas_con_valor = [col for col in agrupadas.columns if col not in columnas_id and 'valor_anterior' not in col.lower()]\n",
    "\n",
    "# =============================================================================\n",
    "# TRANSFORMACIÓN A FORMATO LARGO (MELT) EN CHUNKS\n",
    "# =============================================================================\n",
    "# Para manejar grandes volúmenes de datos sin problemas de memoria,\n",
    "# se realiza el melt en trozos (chunks).\n",
    "\n",
    "chunk_size = 100000  # Ajustar según memoria\n",
    "chunks = [final_columnas_con_valor[i:i+chunk_size] for i in range(0, len(final_columnas_con_valor), chunk_size)]\n",
    "\n",
    "df_long_list = []\n",
    "for chunk in chunks:\n",
    "    df_partial = pd.melt(\n",
    "        agrupadas,\n",
    "        id_vars=columnas_id,\n",
    "        value_vars=chunk,\n",
    "        value_name='Valor',\n",
    "        var_name='Columna_desagregacion'\n",
    "    )\n",
    "    df_long_list.append(df_partial)\n",
    "    del df_partial\n",
    "\n",
    "df_long = pd.concat(df_long_list, ignore_index=True)\n",
    "del df_long_list\n",
    "\n",
    "# Añadir columnas adicionales con información fija\n",
    "df_long = df_long.assign(\n",
    "    Codigo_ISO='DOM',\n",
    "    Categoria='Sector Externo',\n",
    "    Frecuencia='Mensual',\n",
    "    Fuente='ONE',\n",
    "    Pais='República Dominicana',\n",
    "    Departamento='Nacional',\n",
    "    Ciudad='Nacional',\n",
    "    Subcategoria='Importaciones'\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# DEFINICIÓN DE FUNCIONES PARA ASIGNACIÓN DE UNIDAD, TIPO Y AJUSTE\n",
    "# =============================================================================\n",
    "# Estas funciones asignan unidades, tipo de dato y ajuste temporal\n",
    "# según el nombre de la columna desagregada.\n",
    "\n",
    "def columna_unidad(row):\n",
    "    \"\"\"\n",
    "    Determina la unidad de medida a partir del nombre de la variable.\n",
    "    \n",
    "    Ajusta las palabras clave según las variables existentes en tu DataFrame.\n",
    "    Si nunca obtienes '% de las Importaciones totales' o 'USD por tonelada',\n",
    "    verifica los nombres de las columnas o remueve estas condiciones.\n",
    "    \"\"\"\n",
    "    var = str(row['Columna_desagregacion']).lower()\n",
    "\n",
    "    # Ejemplo: Si tus columnas de participación se llaman así:\n",
    "    if 'porcentaje_importaciones_totales' in var or 'participacion_importaciones_totales' in var:\n",
    "        return '% de las Importaciones totales'\n",
    "\n",
    "    # Ejemplo: Si tus columnas de precio por tonelada se llaman así:\n",
    "    elif 'usd_por_ton' in var or 'precio_implicito' in var:\n",
    "        return 'USD por tonelada'\n",
    "\n",
    "    elif 'variacion' in var:\n",
    "        return '% variación anual'\n",
    "\n",
    "    elif 'millones' in var:\n",
    "        return 'Millones'\n",
    "\n",
    "    # Si llegamos hasta aquí es porque el nombre de la variable\n",
    "    # no coincidió con ninguna de las condiciones anteriores.\n",
    "    return 'Unidad desconocida'\n",
    "\n",
    "\n",
    "def columna_tipo(row):\n",
    "    \"\"\"\n",
    "    Determina el tipo de dato (USD FOB, USD CIF, Toneladas métricas, etc.)\n",
    "    en función de la variable.\n",
    "    \n",
    "    Parámetros:\n",
    "        row (pd.Series): Fila del DataFrame, se usa 'Columna_desagregacion' para decidir el tipo.\n",
    "    \n",
    "    Retorno:\n",
    "        str: Tipo correspondiente al valor.\n",
    "    \"\"\"\n",
    "    var = str(row['Columna_desagregacion']).lower()\n",
    "    if 'fob' in var:\n",
    "        return 'USD FOB'\n",
    "    elif 'toneladas' in var and 'variacion' not in var and 'precio_implicito' not in var:\n",
    "        return 'Toneladas métricas'\n",
    "    elif 'variacion' in var:\n",
    "        return 'Precio implícito'\n",
    "    else:\n",
    "        return 'Tipo desconocido'\n",
    "\n",
    "def columna_ajuste(row):\n",
    "    \"\"\"\n",
    "    Determina el ajuste temporal de la serie (mensual o suma móvil 12 meses)\n",
    "    según la variable.\n",
    "    \n",
    "    Parámetros:\n",
    "        row (pd.Series): Fila del DataFrame, se usa 'Columna_desagregacion' para decidir el ajuste.\n",
    "    \n",
    "    Retorno:\n",
    "        str: Ajuste temporal ('Suma móvil 12 meses' o 'Serie mensual').\n",
    "    \"\"\"\n",
    "    var = str(row['Columna_desagregacion']).lower()\n",
    "    if '12m' in var:\n",
    "        return 'Suma móvil 12 meses'\n",
    "    else:\n",
    "        return 'Serie mensual'\n",
    "\n",
    "# Aplicar las funciones de asignación\n",
    "df_long['Unidad'] = df_long.apply(columna_unidad, axis=1)\n",
    "df_long['Tipo'] = df_long.apply(columna_tipo, axis=1)\n",
    "df_long['Ajuste'] = df_long.apply(columna_ajuste, axis=1)\n",
    "\n",
    "# Renombrar columnas de desagregación para mayor claridad\n",
    "df_long.rename(columns={\n",
    "    'Desc_Arancel': 'Desagregacion-4',\n",
    "    'Desc_Partida': 'Desagregacion-3',\n",
    "    'Desc_Capitulo': 'Desagregacion-2',\n",
    "    'Desc_Seccion': 'Desagregacion-1'\n",
    "}, inplace=True)\n",
    "\n",
    "# =============================================================================\n",
    "# RESULTADO FINAL\n",
    "# =============================================================================\n",
    "# 'df_long' contiene los datos en formato largo, con columnas id, valor, unidad,\n",
    "# tipo, ajuste y otras variables descriptivas.\n",
    "\n",
    "print(df_long.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long.sort_values(by=['Fecha', 'Desagregacion-1', 'Desagregacion-2', 'Desagregacion-3','Desagregacion-4'], ascending=True, inplace=True)\n",
    "df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de mapeo: 'columna_original': 'columna_nueva'\n",
    "nuevos_nombres = {\n",
    "    'Fecha': 'Fecha',\n",
    "    'Desagregacion-1': 'Sección',\n",
    "    'Desagregacion-2': 'Capítulo',\n",
    "    'Desagregacion-3': 'Partida',\n",
    "    'Desagregacion-4': 'Arancel',\n",
    "    'Cod_Seccion': 'Código Sección',\n",
    "    'Cod_Capitulo': 'Código Capítulo',\n",
    "    'Cod_Partida': 'Código Partida',\n",
    "    'Cod_Arancel': 'Código Arancel',\n",
    "    'Pais_Origen': 'País Origen',\n",
    "    'Region': 'Región',\n",
    "    'Via_Transporte': 'Vía Transporte',\n",
    "    'Colecturia_ID': 'Colecturía ID',\n",
    "    'Colecturia': 'Colecturía',\n",
    "    'Regimen_Aduanero': 'Régimen Aduanero',\n",
    "    'Valor': 'Valor',\n",
    "    'Codigo_ISO': 'Código ISO',\n",
    "    'Categoria': 'Categoría',\n",
    "    'Frecuencia': 'Frecuencia',\n",
    "    'Pais': 'País',\n",
    "    'Departamento': 'Departamento',\n",
    "    'Subcategoria': 'Sub-Categoría',\n",
    "    'Unidad': 'Unidad',\n",
    "    'Tipo': 'Tipo',\n",
    "    'Ajuste': 'Ajuste',\n",
    "    'Fuente': 'Fuente'\n",
    "}\n",
    "\n",
    "# Aplicar el renombrado de columnas\n",
    "df_long.rename(columns=nuevos_nombres, inplace=True)\n",
    "\n",
    "# Verificar el resultado\n",
    "print(df_long.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long[\"Arancel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_copy = df_long.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_copy.rename(columns={\n",
    "            'Código ISO':'Codigo_ISO',\n",
    "            'Sub-Categoría':'Sub-Categoria',\n",
    "            'Categoría': 'Categoria',\n",
    "            'País': 'Pais',\n",
    "            'Pais_Destino': 'Pais_Origen',\n",
    "            'Régimen Aduanero': 'Regimen_Aduanero',\n",
    "            'Código Sección': 'Codigo_Seccion',\n",
    "            'Sección': 'Seccion',\n",
    "            'Código Arancel': 'Codigo_Arancel',\n",
    "            'Código Capítulo': 'Codigo_Capitulo',\n",
    "            'Código Partida': 'Codigo_Partida',\n",
    "            'Capítulo': 'Capitulo',\n",
    "            'Región': 'Region',\n",
    "            'Vía Transporte': 'Via_Transporte',\n",
    "            'Colecturía ID': 'Colecturia_ID',\n",
    "            'Colecturía': 'Colecturia'\n",
    "        }, inplace=True)\n",
    "\n",
    "df_long_copy.drop(columns=['Mes', 'Columna_desagregacion'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_copy.to_csv('outputs/data_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opciones_unicas_con_conteo(df, columna):\n",
    "    \"\"\"\n",
    "    Muestra todas las opciones únicas de una columna en un DataFrame y \n",
    "    cuántas veces aparece cada una.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame a analizar.\n",
    "        columna (str): El nombre de la columna a verificar.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Serie con los valores únicos y su conteo.\n",
    "    \"\"\"\n",
    "    conteo = df[columna].value_counts(dropna=False)  # Incluye NaN si existen\n",
    "    print(f\"Opciones únicas en la columna '{columna}' y su conteo:\")\n",
    "    for valor, cantidad in conteo.items():\n",
    "        print(f\"- {valor}: {cantidad}\")\n",
    "    return conteo\n",
    "\n",
    "# Ejemplo de uso\n",
    "conteos = opciones_unicas_con_conteo(df_long, 'Columna_desagregacion')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 700000  # Ajustar según la memoria y el tamaño de df_long\n",
    "\n",
    "output_file = 'df_long_output.json'\n",
    "\n",
    "# Abrimos el archivo en modo escritura al iniciar\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    # Iterar sobre el DataFrame en chunks\n",
    "    for i in range(0, len(df_long), chunk_size):\n",
    "        chunk_df = df_long.iloc[i:i+chunk_size]\n",
    "\n",
    "        # Convertir el chunk a JSON line-by-line (orient='records', lines=True)\n",
    "        chunk_json = chunk_df.to_json(orient='records', lines=True, force_ascii=False, date_format='iso', date_unit='s')\n",
    "\n",
    "        # Escribir el chunk en el archivo\n",
    "        f.write(chunk_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_dataframe(df, nombre_base='parte'):\n",
    "    \"\"\"\n",
    "    Divide un DataFrame en 10 partes iguales y guarda cada parte en un CSV.\n",
    "   \n",
    "    Parámetros:\n",
    "    - df: DataFrame a dividir\n",
    "    - nombre_base: prefijo para los nombres de los archivos CSV (por defecto 'parte')\n",
    "    \"\"\"\n",
    "    # Calcula el número de filas por parte\n",
    "    filas_por_parte = len(df) // 25\n",
    "   \n",
    "    # Asegura que se incluyan todas las filas, incluso si no se dividen perfectamente\n",
    "    partes = [df[i:i+filas_por_parte] for i in range(0, len(df), filas_por_parte)]\n",
    "   \n",
    "    # Recorta a 10 partes si hay más\n",
    "    partes = partes[:25]\n",
    "   \n",
    "    # Guarda cada parte en un CSV\n",
    "    for i, parte in enumerate(partes, 1):\n",
    "        nombre_archivo = f'{nombre_base}_{i}.csv'\n",
    "        parte.to_csv('outputs/'+nombre_archivo, index=False)\n",
    "        print(f'Guardado: {nombre_archivo} (filas: {len(parte)})')\n",
    "   \n",
    "dividir_dataframe(df_long,'parte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Definir el mapeo de nombres de columnas\n",
    "column_mapping = {\n",
    "    'Fecha': 'Fecha',\n",
    "    'Sección': 'Seccion',\n",
    "    'Capítulo': 'Capitulo',\n",
    "    'Partida': 'Partida',\n",
    "    'Arancel': 'Arancel',\n",
    "    'Código Sección': 'Codigo_Seccion',\n",
    "    'Código Capítulo': 'Codigo_Capitulo',\n",
    "    'Código Partida': 'Codigo_Partida',\n",
    "    'Código Arancel': 'Codigo_Arancel',\n",
    "    'País Origen': 'Pais_Origen',\n",
    "    'Región': 'Region',\n",
    "    'Vía Transporte': 'Via_Transporte',\n",
    "    'Colecturía ID': 'Colecturia_ID',\n",
    "    'Colecturía': 'Colecturia',\n",
    "    'Régimen Aduanero': 'Regimen_Aduanero',\n",
    "    'Valor': 'Valor',\n",
    "    'Código ISO': 'Codigo_ISO',\n",
    "    'Categoría': 'Categoria',\n",
    "    'Frecuencia': 'Frecuencia',\n",
    "    'País': 'Pais',\n",
    "    'Departamento': 'Departamento',\n",
    "    'Sub-Categoria': 'Sub-Categoria',\n",
    "    'Unidad': 'Unidad',\n",
    "    'Tipo': 'Tipo',\n",
    "    'Ajuste': 'Ajuste',\n",
    "    'Fuente': 'Fuente'\n",
    "}\n",
    "\n",
    "# Leer y renombrar el archivo CSV\n",
    "def rename_csv_columns(input_file, output_file):\n",
    "    # Cargar el archivo CSV\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Renombrar columnas\n",
    "    df.rename(columns=column_mapping, inplace=True)\n",
    "    \n",
    "    # Guardar el CSV corregido\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Archivo procesado y guardado como: {output_file}\")\n",
    "\n",
    "# Procesar archivos CSV en la carpeta actual\n",
    "input_folder = \"csv\"\n",
    "output_folder = \"salida\"\n",
    "\n",
    "# Crear carpeta de salida si no existe\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterar sobre los archivos CSV\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".csv\"):\n",
    "        input_path = os.path.join(input_folder, file)\n",
    "        output_path = os.path.join(output_folder, f\"corregido_{file}\")\n",
    "        rename_csv_columns(input_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_in_chunks(datos, chunk_size, supabase):\n",
    "    # Fetch existing dimensions from the dimension table\n",
    "    def fetch_existing_dimensions():\n",
    "        response = supabase.table('tabladimension').select('*').execute()\n",
    "        \n",
    "        # Si la respuesta no contiene datos, retornamos estructuras vacías\n",
    "        if not response.data or len(response.data) == 0:\n",
    "            return {}, []\n",
    "\n",
    "        # Caso en que hay datos\n",
    "        columns = response.data[0].keys()\n",
    "        dimension_columns = [col for col in columns if col != 'id']\n",
    "\n",
    "        existing_dimensions = {}\n",
    "        for row in response.data:\n",
    "            dimension_combination = tuple(row[col] for col in dimension_columns)\n",
    "            existing_dimensions[dimension_combination] = row['id']\n",
    "\n",
    "        return existing_dimensions, dimension_columns\n",
    "    def get_or_add_dimension(dimension_combination):\n",
    "        if dimension_combination in existing_dimensions:\n",
    "            return existing_dimensions[dimension_combination]\n",
    "        else:\n",
    "            # Add new dimension to the dimension table\n",
    "            new_dimension_data = dict(zip(dimension_columns, dimension_combination))\n",
    "            print(new_dimension_data)\n",
    "            response = supabase.table('tabladimension').insert(new_dimension_data).execute()\n",
    "            new_id = response.data[0]['id']\n",
    "            existing_dimensions[dimension_combination] = new_id\n",
    "            return new_id\n",
    " \n",
    "    def create_dimension_combination(row):\n",
    "        return tuple(row[col] for col in dimension_columns)\n",
    "   \n",
    "    existing_dimensions, dimension_columns = fetch_existing_dimensions()\n",
    "    errores = []  # To track errors\n",
    "    total_chunks = (len(datos) + chunk_size - 1) // chunk_size\n",
    "    # Define columns to keep in the final upsert\n",
    "    columns_to_keep = ['Fecha', 'Valor', 'id']\n",
    "   \n",
    "    for i in range(total_chunks):\n",
    "        start_index = i * chunk_size\n",
    "        end_index = (i + 1) * chunk_size\n",
    "        chunk_data = datos[start_index:end_index]\n",
    " \n",
    "        # Prepare data for upsert\n",
    "        for row in chunk_data:\n",
    "            dimension_combination = create_dimension_combination(row)\n",
    "            dimension_id = get_or_add_dimension(dimension_combination)\n",
    "            row['id'] = dimension_id\n",
    " \n",
    "        # Drop columns not in the columns_to_keep\n",
    "        chunk_data_filtered = [{k: row[k] for k in columns_to_keep} for row in chunk_data]\n",
    " \n",
    "        # Upsert data into the main table\n",
    "        try:\n",
    "            response = supabase.table('datos_light').upsert(chunk_data_filtered).execute()\n",
    "            print(response)\n",
    "        except Exception as e:\n",
    "            errores.append(str(e))\n",
    " \n",
    "    return f\"Terminamos errores: {', '.join(errores)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "def get_secret():\n",
    " \n",
    "    secret_name = \"RepDom-DB\"\n",
    "    region_name = \"us-east-1\"\n",
    " \n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    " \n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        # For a list of exceptions thrown, see\n",
    "        # https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "        raise e\n",
    " \n",
    "    secret = json.loads(get_secret_value_response['SecretString'])\n",
    "    return secret\n",
    "\n",
    "\n",
    "secretos=get_secret()\n",
    "\n",
    "url_supabase: str = secretos['supabase_URL_RD']\n",
    "key: str = secretos['supabase_key_RD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df=pd.read_csv(\"salida\\corregido_parte_1.csv\") \n",
    "df = df.dropna(subset=['Fecha', 'Valor']).drop_duplicates()\n",
    "\n",
    "# Convierte las columnas a cadena de texto si es necesario\n",
    "for col in df.columns:\n",
    "    if col != 'Valor':  # 'Valor' debe mantenerse en su tipo original\n",
    "        df[col] = df[col].astype(str)\n",
    "\n",
    "# Convierte a diccionario\n",
    "datos = df.to_dict(orient='records')\n",
    "supabase: Client = create_client(url_supabase, key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datos[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data_in_chunks(test_data,10,supabase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer la primera fila del DataFrame\n",
    "row = df.iloc[0].to_dict()\n",
    "print(\"\\nPrimera fila del DataFrame:\", row)\n",
    "\n",
    "# Función fetch_existing_dimensions (probamos si funciona correctamente)\n",
    "def fetch_existing_dimensions():\n",
    "    print(\"\\n--- Ejecutando fetch_existing_dimensions ---\") \n",
    "    response = supabase.table('tabladimension').select('*').execute()\n",
    "    print(\"Response data:\", response.data)\n",
    "\n",
    "    if not response.data or len(response.data) == 0:\n",
    "        # Si no hay datos en la tabla, definimos las columnas manualmente\n",
    "        dimension_columns = ['Seccion', 'Capitulo', 'Partida', 'Arancel', 'Codigo_Seccion', \n",
    "                             'Codigo_Capitulo', 'Codigo_Partida', 'Codigo_Arancel', \n",
    "                             'Pais_Origen', 'Region', 'Via_Transporte', 'Colecturia_ID', \n",
    "                             'Colecturia', 'Regimen_Aduanero', 'Codigo_ISO', 'Categoria', \n",
    "                             'Frecuencia', 'Pais', 'Departamento', 'Sub-Categoria', \n",
    "                             'Unidad', 'Tipo', 'Ajuste', 'Fuente']\n",
    "        return {}, dimension_columns\n",
    "\n",
    "    columns = response.data[0].keys()\n",
    "    dimension_columns = [col for col in columns if col != 'id']\n",
    "\n",
    "    existing_dimensions = {}\n",
    "    for r in response.data:\n",
    "        dimension_combination = tuple(r[c] for c in dimension_columns)\n",
    "        existing_dimensions[dimension_combination] = r['id']\n",
    "\n",
    "    print(\"Existing dimensions:\", existing_dimensions)\n",
    "    print(\"Dimension columns:\", dimension_columns)\n",
    "    return existing_dimensions, dimension_columns\n",
    "\n",
    "# Función create_dimension_combination\n",
    "def create_dimension_combination(row):\n",
    "    print(\"\\n--- Ejecutando create_dimension_combination ---\")\n",
    "    combination = tuple(row[col] for col in dimension_columns)\n",
    "    print(\"Dimension combination:\", combination)\n",
    "    return combination\n",
    "\n",
    "# Función get_or_add_dimension\n",
    "def get_or_add_dimension(dimension_combination):\n",
    "    print(\"\\n--- Ejecutando get_or_add_dimension ---\")\n",
    "    if dimension_combination in existing_dimensions:\n",
    "        print(\"ID existente encontrado:\", existing_dimensions[dimension_combination])\n",
    "        return existing_dimensions[dimension_combination]\n",
    "    else:\n",
    "        new_dimension_data = dict(zip(dimension_columns, dimension_combination))\n",
    "        print(\"Insertando nueva dimensión:\", new_dimension_data)\n",
    "        response = supabase.table('tabladimension').insert(new_dimension_data).execute()\n",
    "        new_id = response.data[0]['id']\n",
    "        print(\"Nuevo ID retornado:\", new_id)\n",
    "        existing_dimensions[dimension_combination] = new_id\n",
    "        return new_id\n",
    "\n",
    "# Proceso paso a paso con la primera fila\n",
    "print(\"\\n--- INICIANDO PRUEBA PASO A PASO ---\")\n",
    "\n",
    "# Paso 1: Fetch existing dimensions\n",
    "existing_dimensions, dimension_columns = fetch_existing_dimensions()\n",
    "\n",
    "# Paso 2: Crear la combinación de dimensiones\n",
    "dimension_combination = create_dimension_combination(row)\n",
    "\n",
    "# Paso 3: Obtener o agregar la dimensión\n",
    "dimension_id = get_or_add_dimension(dimension_combination)\n",
    "\n",
    "# Paso 4: Preparar datos para upsert\n",
    "row['id'] = dimension_id\n",
    "columns_to_keep = ['Fecha', 'Valor', 'id']\n",
    "chunk_data_filtered = {k: row[k] for k in columns_to_keep}\n",
    "print(\"\\nDatos preparados para upsert:\", chunk_data_filtered)\n",
    "\n",
    "# Paso 5: Realizar el upsert en Supabase\n",
    "try:\n",
    "    print(\"\\n--- Ejecutando upsert en datos_light ---\")\n",
    "    response = supabase.table('datos_light').upsert([chunk_data_filtered]).execute()\n",
    "    print(\"Upsert response:\", response.data)\n",
    "except Exception as e:\n",
    "    print(\"Error durante el upsert:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisar y eliminar filas con valores NaN en las columnas Fecha o Valor\n",
    "def clean_dataframe(df):\n",
    "    print(\"--- Revisión de valores NaN ---\")\n",
    "    \n",
    "    # Contar los NaN en las columnas relevantes\n",
    "    print(\"Conteo de NaN antes de limpiar:\")\n",
    "    print(df[['Fecha', 'Valor']].isnull().sum())\n",
    "    \n",
    "    # Eliminar filas con NaN en las columnas 'Fecha' o 'Valor'\n",
    "    df_cleaned = df.dropna(subset=['Fecha', 'Valor'])\n",
    "    \n",
    "    # Verificar nuevamente\n",
    "    print(\"\\nConteo de NaN después de limpiar:\")\n",
    "    print(df_cleaned[['Fecha', 'Valor']].isnull().sum())\n",
    "    \n",
    "    # Mostrar cuántas filas se eliminaron\n",
    "    rows_removed = len(df) - len(df_cleaned)\n",
    "    print(f\"\\nSe eliminaron {rows_removed} filas con NaN en 'Fecha' o 'Valor'.\")\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "df = clean_dataframe(df)\n",
    "\n",
    "# Opcional: Guardar el DataFrame limpio para futuras referencias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "supabase: Client = create_client(url_supabase, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el DataFrame\n",
    "def split_dataframe(df):\n",
    "    \"\"\"\n",
    "    Divide el DataFrame en dos:\n",
    "    1. Dimensiones fijas (tabladimension)\n",
    "    2. Datos dinámicos (datos_light)\n",
    "    \"\"\"\n",
    "    dimension_columns = [col for col in df.columns if col not in ['Fecha', 'Valor']]\n",
    "\n",
    "    # Extraer dimensiones únicas\n",
    "    dimensiones = df[dimension_columns].drop_duplicates().reset_index(drop=True)\n",
    "    dimensiones['id'] = range(1, len(dimensiones) + 1)  # Agregar IDs únicos\n",
    "\n",
    "    # Mapear IDs a los datos dinámicos\n",
    "    datos = df.merge(dimensiones, on=dimension_columns, how='left')[['Fecha', 'Valor', 'id']]\n",
    "\n",
    "    return dimensiones, datos\n",
    "\n",
    "# Cargar archivo CSV\n",
    "df = pd.read_csv('salida/corregido_parte_1.csv')\n",
    "df = df.dropna(subset=['Fecha', 'Valor']).drop_duplicates()\n",
    "\n",
    "# Dividir el DataFrame en dimensiones y datos\n",
    "dimensiones, datos = split_dataframe(df)\n",
    "\n",
    "# Guardar los DataFrames resultantes en archivos CSV\n",
    "dimensiones.to_csv('prueba/dimensiones.csv', index=False)\n",
    "datos.to_csv('prueba/datos.csv', index=False)\n",
    "\n",
    "print(\"Dimensiones y datos divididos y guardados como CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Cargar y dividir un DataFrame\n",
    "def split_dataframe(df):\n",
    "    \"\"\"\n",
    "    Divide el DataFrame en dos:\n",
    "    1. Dimensiones fijas (tabladimension)\n",
    "    2. Datos dinámicos (datos_light)\n",
    "    \"\"\"\n",
    "    dimension_columns = [col for col in df.columns if col not in ['Fecha', 'Valor']]\n",
    "\n",
    "    # Extraer dimensiones únicas\n",
    "    dimensiones = df[dimension_columns].drop_duplicates().reset_index(drop=True)\n",
    "    dimensiones['id'] = range(1, len(dimensiones) + 1)  # Agregar IDs únicos\n",
    "\n",
    "    # Mapear IDs a los datos dinámicos\n",
    "    datos = df.merge(dimensiones, on=dimension_columns, how='left')[['Fecha', 'Valor', 'id']]\n",
    "\n",
    "    return dimensiones, datos\n",
    "\n",
    "def process_multiple_csv(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Procesa todos los archivos CSV en una carpeta, dividiéndolos en dimensiones y datos.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    archivos = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "    all_dimensiones = []\n",
    "    all_datos = []\n",
    "\n",
    "    for archivo in archivos:\n",
    "        print(f\"Procesando: {archivo}\")\n",
    "        # Cargar archivo CSV\n",
    "        df = pd.read_csv(os.path.join(input_folder, archivo))\n",
    "        df = df.dropna(subset=['Fecha', 'Valor']).drop_duplicates()\n",
    "\n",
    "        # Dividir el DataFrame\n",
    "        dimensiones, datos = split_dataframe(df)\n",
    "\n",
    "        # Agregar a las listas generales\n",
    "        all_dimensiones.append(dimensiones)\n",
    "        all_datos.append(datos)\n",
    "\n",
    "    # Combinar y guardar dimensiones únicas\n",
    "    combined_dimensiones = pd.concat(all_dimensiones).drop_duplicates().reset_index(drop=True)\n",
    "    combined_dimensiones.to_csv(os.path.join(output_folder, 'dimensiones.csv'), index=False)\n",
    "\n",
    "    # Combinar y guardar datos\n",
    "    combined_datos = pd.concat(all_datos).reset_index(drop=True)\n",
    "    combined_datos.to_csv(os.path.join(output_folder, 'datos.csv'), index=False)\n",
    "\n",
    "    print(\"Todos los archivos han sido procesados y guardados.\")\n",
    "\n",
    "# Procesar todos los CSV en la carpeta 'entrada' y guardar en la carpeta 'salida'\n",
    "process_multiple_csv('entrada', 'salida')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Cargar y dividir un DataFrame\n",
    "def split_dataframe(df):\n",
    "    \"\"\"\n",
    "    Divide el DataFrame en dos:\n",
    "    1. Dimensiones fijas (tabladimension)\n",
    "    2. Datos dinámicos (datos_light)\n",
    "    \"\"\"\n",
    "    dimension_columns = [col for col in df.columns if col not in ['Fecha', 'Valor']]\n",
    "\n",
    "    # Extraer dimensiones únicas\n",
    "    dimensiones = df[dimension_columns].drop_duplicates().reset_index(drop=True)\n",
    "    dimensiones['id'] = range(1, len(dimensiones) + 1)  # Agregar IDs únicos\n",
    "\n",
    "    # Mapear IDs a los datos dinámicos\n",
    "    datos = df.merge(dimensiones, on=dimension_columns, how='left')[['Fecha', 'Valor', 'id']]\n",
    "\n",
    "    return dimensiones, datos\n",
    "\n",
    "def process_multiple_csv(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Procesa todos los archivos CSV en una carpeta, dividiéndolos en dimensiones y datos.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    archivos = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "    all_dimensiones = []\n",
    "    all_datos = []\n",
    "\n",
    "    for archivo in archivos:\n",
    "        print(f\"Procesando: {archivo}\")\n",
    "        # Cargar archivo CSV\n",
    "        df = pd.read_csv(os.path.join(input_folder, archivo))\n",
    "\n",
    "        # Eliminar filas con valores nulos en 'Fecha' o 'Valor'\n",
    "        df = df.dropna(subset=['Fecha', 'Valor']).drop_duplicates()\n",
    "\n",
    "        # Dividir el DataFrame\n",
    "        dimensiones, datos = split_dataframe(df)\n",
    "\n",
    "        # Agregar a las listas generales\n",
    "        all_dimensiones.append(dimensiones)\n",
    "        all_datos.append(datos)\n",
    "\n",
    "    # Combinar y guardar dimensiones únicas\n",
    "    combined_dimensiones = pd.concat(all_dimensiones).drop_duplicates().reset_index(drop=True)\n",
    "    combined_dimensiones.to_csv(os.path.join(output_folder, 'dimensiones.csv'), index=False)\n",
    "\n",
    "    # Combinar y guardar datos\n",
    "    combined_datos = pd.concat(all_datos).reset_index(drop=True)\n",
    "    combined_datos.to_csv(os.path.join(output_folder, 'datos.csv'), index=False)\n",
    "\n",
    "    print(\"Todos los archivos han sido procesados y guardados.\")\n",
    "\n",
    "def verify_id_counts(datos_file):\n",
    "    \"\"\"\n",
    "    Verifica cuántas filas tienen el mismo ID en el archivo de datos.\n",
    "    \"\"\"\n",
    "    datos = pd.read_csv(datos_file)\n",
    "    id_counts = datos['id'].value_counts()\n",
    "    print(\"Conteo de filas por ID:\")\n",
    "    print(id_counts)\n",
    "    return id_counts\n",
    "\n",
    "# Procesar todos los CSV en la carpeta 'entrada' y guardar en la carpeta 'salida'\n",
    "process_multiple_csv('csv_bien', 'prueba')\n",
    "\n",
    "# Verificar los IDs en el archivo 'datos.csv'\n",
    "verify_id_counts('prueba/datos.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer la primera fila del DataFrame\n",
    "row = df.iloc[0].to_dict()\n",
    "print(\"\\nPrimera fila del DataFrame:\", row)\n",
    "\n",
    "# Función fetch_existing_dimensions (probamos si funciona correctamente)\n",
    "def fetch_existing_dimensions():\n",
    "    print(\"\\n--- Ejecutando fetch_existing_dimensions ---\") \n",
    "    response = supabase.table('tabladimension').select('*').execute()\n",
    "    print(\"Response data:\", response.data)\n",
    "\n",
    "    if not response.data or len(response.data) == 0:\n",
    "        # Si no hay datos en la tabla, definimos las columnas manualmente\n",
    "        dimension_columns = ['Seccion', 'Capitulo', 'Partida', 'Arancel', 'Codigo_Seccion', \n",
    "                             'Codigo_Capitulo', 'Codigo_Partida', 'Codigo_Arancel', \n",
    "                             'Pais_Origen', 'Region', 'Via_Transporte', 'Colecturia_ID', \n",
    "                             'Colecturia', 'Regimen_Aduanero', 'Codigo_ISO', 'Categoria', \n",
    "                             'Frecuencia', 'Pais', 'Departamento', 'Sub-Categoria', \n",
    "                             'Unidad', 'Tipo', 'Ajuste', 'Fuente']\n",
    "        return {}, dimension_columns\n",
    "\n",
    "    columns = response.data[0].keys()\n",
    "    dimension_columns = [col for col in columns if col != 'id']\n",
    "\n",
    "    existing_dimensions = {}\n",
    "    for r in response.data:\n",
    "        dimension_combination = tuple(r[c] for c in dimension_columns)\n",
    "        existing_dimensions[dimension_combination] = r['id']\n",
    "\n",
    "    print(\"Existing dimensions:\", existing_dimensions)\n",
    "    print(\"Dimension columns:\", dimension_columns)\n",
    "    return existing_dimensions, dimension_columns\n",
    "\n",
    "# Función create_dimension_combination\n",
    "def create_dimension_combination(row):\n",
    "    print(\"\\n--- Ejecutando create_dimension_combination ---\")\n",
    "    combination = tuple(row[col] for col in dimension_columns)\n",
    "    print(\"Dimension combination:\", combination)\n",
    "    return combination\n",
    "\n",
    "# Función get_or_add_dimension\n",
    "def get_or_add_dimension(dimension_combination):\n",
    "    print(\"\\n--- Ejecutando get_or_add_dimension ---\")\n",
    "    if dimension_combination in existing_dimensions:\n",
    "        print(\"ID existente encontrado:\", existing_dimensions[dimension_combination])\n",
    "        return existing_dimensions[dimension_combination]\n",
    "    else:\n",
    "        new_dimension_data = dict(zip(dimension_columns, dimension_combination))\n",
    "        print(\"Insertando nueva dimensión:\", new_dimension_data)\n",
    "        response = supabase.table('tabladimension').insert(new_dimension_data).execute()\n",
    "        new_id = response.data[0]['id']\n",
    "        print(\"Nuevo ID retornado:\", new_id)\n",
    "        existing_dimensions[dimension_combination] = new_id\n",
    "        return new_id\n",
    "\n",
    "# Proceso paso a paso con la primera fila\n",
    "print(\"\\n--- INICIANDO PRUEBA PASO A PASO ---\")\n",
    "\n",
    "# Paso 1: Fetch existing dimensions\n",
    "existing_dimensions, dimension_columns = fetch_existing_dimensions()\n",
    "\n",
    "# Paso 2: Crear la combinación de dimensiones\n",
    "dimension_combination = create_dimension_combination(row)\n",
    "\n",
    "# Paso 3: Obtener o agregar la dimensión\n",
    "dimension_id = get_or_add_dimension(dimension_combination)\n",
    "\n",
    "# Paso 4: Preparar datos para upsert\n",
    "row['id'] = dimension_id\n",
    "columns_to_keep = ['Fecha', 'Valor', 'id']\n",
    "chunk_data_filtered = {k: row[k] for k in columns_to_keep}\n",
    "print(\"\\nDatos preparados para upsert:\", chunk_data_filtered)\n",
    "\n",
    "# Paso 5: Realizar el upsert en Supabase\n",
    "try:\n",
    "    print(\"\\n--- Ejecutando upsert en datos_light ---\")\n",
    "    response = supabase.table('datos_light').upsert([chunk_data_filtered]).execute()\n",
    "    print(\"Upsert response:\", response.data)\n",
    "except Exception as e:\n",
    "    print(\"Error durante el upsert:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Función para normalizar un chunk de datos\n",
    "def normalize_chunk(chunk, dimension_columns):\n",
    "    \"\"\"\n",
    "    Normaliza las columnas relevantes en un chunk.\n",
    "    \"\"\"\n",
    "    chunk[dimension_columns] = chunk[dimension_columns].apply(lambda x: x.astype(str).str.strip().str.lower())\n",
    "    return chunk\n",
    "\n",
    "# Función para procesar un chunk de datos\n",
    "def process_chunk(chunk, dimension_columns):\n",
    "    \"\"\"\n",
    "    Procesa un chunk, extrae dimensiones únicas y mapea IDs.\n",
    "    \"\"\"\n",
    "    dimensiones = chunk[dimension_columns].drop_duplicates()\n",
    "    dimensiones['id'] = range(1, len(dimensiones) + 1)\n",
    "    datos = chunk.merge(dimensiones, on=dimension_columns, how='left')[['Fecha', 'Valor', 'id']]\n",
    "    return dimensiones, datos\n",
    "\n",
    "# Función para procesar un archivo CSV en chunks\n",
    "def process_single_csv(file_path, output_folder, chunk_size=100000):\n",
    "    \"\"\"\n",
    "    Procesa un archivo CSV en chunks y guarda los resultados parciales.\n",
    "    \"\"\"\n",
    "    print(f\"Procesando archivo: {file_path}\")\n",
    "\n",
    "    dimensiones_file = os.path.join(output_folder, 'dimensiones_temp.csv')\n",
    "    datos_file = os.path.join(output_folder, 'datos_temp.csv')\n",
    "\n",
    "    for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n",
    "        # Modificar la columna 'Codigo_ISO' a 'DOM'\n",
    "        if 'Codigo_ISO' in chunk.columns:\n",
    "            chunk['Codigo_ISO'] = 'DOM'\n",
    "\n",
    "        # Añadir la columna 'Ciudad'\n",
    "        chunk['Ciudad'] = \"Régimen de aduanas\"\n",
    "\n",
    "        # Eliminar valores nulos y normalizar\n",
    "        chunk = chunk.dropna(subset=['Fecha', 'Valor']).drop_duplicates()\n",
    "        dimension_columns = [col for col in chunk.columns if col not in ['Fecha', 'Valor']]\n",
    "        chunk = normalize_chunk(chunk, dimension_columns)\n",
    "\n",
    "        # Procesar el chunk\n",
    "        dimensiones, datos = process_chunk(chunk, dimension_columns)\n",
    "\n",
    "        # Guardar resultados parciales\n",
    "        if not os.path.exists(dimensiones_file):\n",
    "            dimensiones.to_csv(dimensiones_file, index=False)\n",
    "        else:\n",
    "            dimensiones.to_csv(dimensiones_file, mode='a', header=False, index=False)\n",
    "\n",
    "        if not os.path.exists(datos_file):\n",
    "            datos.to_csv(datos_file, index=False)\n",
    "        else:\n",
    "            datos.to_csv(datos_file, mode='a', header=False, index=False)\n",
    "\n",
    "# Función para consolidar resultados parciales\n",
    "def consolidate_results(output_folder):\n",
    "    \"\"\"\n",
    "    Consolida los resultados parciales y genera los archivos finales.\n",
    "    \"\"\"\n",
    "    dimensiones_file = os.path.join(output_folder, 'dimensiones_temp.csv')\n",
    "    datos_file = os.path.join(output_folder, 'datos_temp.csv')\n",
    "\n",
    "    if os.path.exists(dimensiones_file):\n",
    "        dimensiones = pd.read_csv(dimensiones_file).drop_duplicates().reset_index(drop=True)\n",
    "        dimensiones.to_csv(os.path.join(output_folder, 'dimensiones.csv'), index=False)\n",
    "        os.remove(dimensiones_file)\n",
    "\n",
    "    if os.path.exists(datos_file):\n",
    "        datos = pd.read_csv(datos_file).reset_index(drop=True)\n",
    "        datos.to_csv(os.path.join(output_folder, 'datos.csv'), index=False)\n",
    "        os.remove(datos_file)\n",
    "\n",
    "# Función para procesar múltiples archivos CSV\n",
    "def process_multiple_csv(input_folder, output_folder, chunk_size=100000):\n",
    "    \"\"\"\n",
    "    Procesa todos los archivos CSV en una carpeta.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    archivos = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "    for archivo in archivos:\n",
    "        file_path = os.path.join(input_folder, archivo)\n",
    "        process_single_csv(file_path, output_folder, chunk_size)\n",
    "\n",
    "    consolidate_results(output_folder)\n",
    "\n",
    "# Ejecutar el proceso\n",
    "process_multiple_csv('csv_bien', 'prueba')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('prueba/dimensiones.csv')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUBIDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_final=pd.read_csv('completo_inicial.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una copia del DataFrame original\n",
    "working_df = df_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# =============================================================================\n",
    "# 1. CONFIGURACIÓN INICIAL\n",
    "# =============================================================================\n",
    "# Este bloque establece columnas iniciales con valores estáticos y ordena el DataFrame.\n",
    "# Se asume que 'working_df' ya está definido anteriormente.\n",
    "\n",
    "# Añadir columnas iniciales con valores estáticos\n",
    "working_df['Código ISO'] = 'DOM'\n",
    "working_df['País'] = 'República Dominicana'\n",
    "working_df['Departamento'] = 'Nacional'\n",
    "working_df['Ciudad'] = 'Nacional'\n",
    "working_df['Categoría'] = 'Sector Externo'\n",
    "working_df['Sub-Categoría'] = 'Importaciones'\n",
    "\n",
    "# Ordenar las columnas en el DataFrame (ajusta si alguna no existe en tu DF)\n",
    "column_order = [\n",
    "    'Código ISO', 'País', 'Departamento', 'Ciudad', 'Categoría', 'Sub-Categoría',\n",
    "    'Pais_Origen', 'Region', 'Via_Transporte', 'Colecturia', 'Regimen_Aduanero',\n",
    "    'Cod_Seccion', 'Cod_Capitulo', 'Cod_Partida', 'Cod_Arancel',\n",
    "    'Transacción', 'Año', 'Mes', 'Desc_Seccion', 'Desc_Capitulo',\n",
    "    'Desc_Partida', 'Desc_Arancel', 'Peso', 'V_FOB_USD', 'V_CIF_USD',\n",
    "    'Colecturia_ID'\n",
    "]\n",
    "working_df = working_df[column_order]\n",
    "\n",
    "# =============================================================================\n",
    "# 2. PREPROCESAMIENTO DE DATOS\n",
    "# =============================================================================\n",
    "# Limpiar guiones \"-\" en las columnas descriptivas y generar métricas base.\n",
    "# También se crea la columna 'Fecha' y se eliminan 'Año'/'Mes'.\n",
    "\n",
    "for col in ['Desc_Seccion', 'Desc_Capitulo', 'Desc_Partida', 'Desc_Arancel']:\n",
    "    working_df[col] = working_df[col].str.replace('-', ' ').str.strip()\n",
    "\n",
    "# Crear columnas derivadas\n",
    "working_df['Toneladas'] = working_df['Peso'] / 1000.0\n",
    "working_df['Millones_USD_FOB'] = working_df['V_FOB_USD'] / 1_000_000\n",
    "working_df['Millones_Toneladas_metricas'] = working_df['Toneladas'] / 1_000_000\n",
    "working_df['Millones_USD_CIF'] = working_df['V_CIF_USD'] / 1_000_000\n",
    "\n",
    "# Crear columna 'Fecha' a partir de 'Año' y 'Mes'\n",
    "working_df['Fecha'] = pd.to_datetime(\n",
    "    working_df['Año'].astype(str) + '-' + working_df['Mes'].apply(lambda x: f'{x:02}') + '-01'\n",
    ")\n",
    "working_df.drop(['Año', 'Mes'], axis=1, inplace=True)\n",
    "\n",
    "# Ordenar por criterios lógicos\n",
    "working_df.sort_values(\n",
    "    by=['Fecha', 'Desc_Seccion', 'Desc_Capitulo', 'Desc_Partida', 'Desc_Arancel'],\n",
    "    ascending=True,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. AGRUPACIÓN MENSUAL Y SUMA MÓVIL 12 MESES\n",
    "# =============================================================================\n",
    "agrupadas = working_df.groupby(\n",
    "    [\n",
    "        'Fecha', 'Desc_Seccion', 'Desc_Capitulo', 'Desc_Partida', 'Desc_Arancel',\n",
    "        'Cod_Seccion', 'Cod_Capitulo', 'Cod_Partida', 'Cod_Arancel'\n",
    "    ],\n",
    "    as_index=False\n",
    ").agg({\n",
    "    'Toneladas': 'sum',\n",
    "    'Millones_USD_FOB': 'sum',\n",
    "    'Millones_Toneladas_metricas': 'sum',\n",
    "    'Millones_USD_CIF': 'sum',\n",
    "    'Pais_Origen': 'first',\n",
    "    'Region': 'first',\n",
    "    'Via_Transporte': 'first',\n",
    "    'Colecturia_ID': 'first',\n",
    "    'Colecturia': 'first',\n",
    "    'Regimen_Aduanero': 'first'\n",
    "})\n",
    "\n",
    "# Extraer mes para rolling y variación\n",
    "agrupadas['Mes'] = agrupadas['Fecha'].dt.month\n",
    "\n",
    "# Rolling 12M\n",
    "grouped = agrupadas.groupby(['Desc_Seccion', 'Desc_Capitulo', 'Desc_Partida'])\n",
    "agrupadas['Toneladas_12M'] = grouped['Toneladas'].transform(lambda x: x.rolling(12, min_periods=1).sum())\n",
    "agrupadas['Millones_USD_FOB_12M'] = grouped['Millones_USD_FOB'].transform(lambda x: x.rolling(12, min_periods=1).sum())\n",
    "agrupadas['Millones_Toneladas_metricas_12M'] = grouped['Millones_Toneladas_metricas'].transform(lambda x: x.rolling(12, min_periods=1).sum())\n",
    "agrupadas['Millones_USD_CIF_12M'] = grouped['Millones_USD_CIF'].transform(lambda x: x.rolling(12, min_periods=1).sum())\n",
    "\n",
    "# =============================================================================\n",
    "# 4. VALOR ANTERIOR (AÑO PREVIO) Y VARIACIÓN\n",
    "# =============================================================================\n",
    "columnas_id = [\n",
    "    'Fecha', 'Desc_Seccion', 'Desc_Capitulo', 'Desc_Partida', 'Desc_Arancel',\n",
    "    'Cod_Seccion', 'Cod_Capitulo', 'Cod_Partida', 'Cod_Arancel', 'Mes',\n",
    "    'Pais_Origen', 'Region', 'Via_Transporte', 'Colecturia_ID', 'Colecturia',\n",
    "    'Regimen_Aduanero'\n",
    "]\n",
    "columnas_con_valor = [c for c in agrupadas.columns if c not in columnas_id]\n",
    "\n",
    "for col in columnas_con_valor:\n",
    "    agrupadas[f'{col}_valor_anterior'] = agrupadas.groupby('Mes')[col].shift(12)\n",
    "    agrupadas[f'variacion {col}'] = (\n",
    "        (agrupadas[col] - agrupadas[f'{col}_valor_anterior']) /\n",
    "        agrupadas[f'{col}_valor_anterior'] * 100\n",
    "    )\n",
    "\n",
    "# Excluir columnas *_valor_anterior antes de melt\n",
    "final_columnas_con_valor = [\n",
    "    c for c in agrupadas.columns\n",
    "    if c not in columnas_id and 'valor_anterior' not in c.lower()\n",
    "]\n",
    "\n",
    "# =============================================================================\n",
    "# 5. TRANSFORMACIÓN A LARGO (MELT) CON CHUNKING POR FILAS\n",
    "# =============================================================================\n",
    "chunksize = 50_000\n",
    "num_rows = len(agrupadas)\n",
    "df_long_list = []\n",
    "\n",
    "for start in range(0, num_rows, chunksize):\n",
    "    end = start + chunksize\n",
    "    subset = agrupadas.iloc[start:end].copy()\n",
    "\n",
    "    df_partial = pd.melt(\n",
    "        subset,\n",
    "        id_vars=columnas_id,\n",
    "        value_vars=final_columnas_con_valor,\n",
    "        value_name='Valor',            # <--- AQUÍ se crea la columna 'Valor'\n",
    "        var_name='Columna_desagregacion'\n",
    "    )\n",
    "    df_long_list.append(df_partial)\n",
    "    del df_partial\n",
    "    del subset\n",
    "\n",
    "df_long = pd.concat(df_long_list, ignore_index=True)\n",
    "del df_long_list\n",
    "\n",
    "# =============================================================================\n",
    "# 6. AÑADIR COLUMNAS EXTRAS (CON INFORMACIÓN FIJA) Y FUNCIONES\n",
    "# =============================================================================\n",
    "df_long = df_long.assign(\n",
    "    Codigo_ISO='DOM',\n",
    "    Categoria='Sector Externo',\n",
    "    Frecuencia='Mensual',\n",
    "    Fuente='ONE',\n",
    "    Pais='República Dominicana',\n",
    "    Departamento='Nacional',\n",
    "    Ciudad='Nacional',\n",
    "    Sub_Categoria='Importaciones'\n",
    ")\n",
    "\n",
    "# Funciones para Unidad, Tipo, Ajuste\n",
    "def columna_unidad(row):\n",
    "    var = str(row['Columna_desagregacion']).lower()\n",
    "    if 'porcentaje_importaciones_totales' in var or 'participacion_importaciones_totales' in var:\n",
    "        return '% de las Importaciones totales'\n",
    "    elif 'usd_por_ton' in var or 'precio_implicito' in var:\n",
    "        return 'USD por tonelada'\n",
    "    elif 'variacion' in var:\n",
    "        return '% variación anual'\n",
    "    elif 'millones' in var:\n",
    "        return 'Millones'\n",
    "    return 'Unidad desconocida'\n",
    "\n",
    "def columna_tipo(row):\n",
    "    var = str(row['Columna_desagregacion']).lower()\n",
    "    if 'fob' in var:\n",
    "        return 'USD FOB'\n",
    "    elif 'cif' in var:\n",
    "        return 'USD CIF'\n",
    "    elif 'toneladas' in var and 'variacion' not in var:\n",
    "        return 'Toneladas métricas'\n",
    "    elif 'variacion' in var:\n",
    "        return 'Precio implícito'\n",
    "    return 'Tipo desconocido'\n",
    "\n",
    "def columna_ajuste(row):\n",
    "    var = str(row['Columna_desagregacion']).lower()\n",
    "    if '12m' in var:\n",
    "        return 'Suma móvil 12 meses'\n",
    "    return 'Serie mensual'\n",
    "\n",
    "df_long['Unidad'] = df_long.apply(columna_unidad, axis=1)\n",
    "df_long['Tipo'] = df_long.apply(columna_tipo, axis=1)\n",
    "df_long['Ajuste'] = df_long.apply(columna_ajuste, axis=1)\n",
    "\n",
    "# =============================================================================\n",
    "# 7. RENOMBRAR COLUMNAS AL FORMATO DEFINITIVO\n",
    "# =============================================================================\n",
    "\n",
    "# Creamos un diccionario de renombrado para que tus columnas finales\n",
    "# queden en los nombres requeridos:\n",
    "rename_dict = {\n",
    "    # Desagregaciones colapsadas (después del melt):\n",
    "    'Desagregacion-1': 'Seccion',\n",
    "    'Desagregacion-2': 'Capitulo',\n",
    "    'Desagregacion-3': 'Partida',\n",
    "    'Desagregacion-4': 'Arancel',\n",
    "\n",
    "    # Códigos\n",
    "    'Cod_Seccion': 'Codigo_Seccion',\n",
    "    'Cod_Arancel': 'Codigo_Arancel',\n",
    "    'Cod_Capitulo': 'Codigo_Capitulo',\n",
    "    'Cod_Partida': 'Codigo_Partida',\n",
    "\n",
    "    # Subcategoría\n",
    "    'Sub_Categoria': 'Sub-Categoria',\n",
    "\n",
    "    # Regimen\n",
    "    'Regimen_Aduanero': 'Regimen_Adicional'\n",
    "}\n",
    "\n",
    "df_long.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# =============================================================================\n",
    "# 8. REORDENAR COLUMNAS SEGÚN LISTA SOLICITADA\n",
    "# =============================================================================\n",
    "ordered_columns = [\n",
    "    'Fecha',\n",
    "    'Valor',\n",
    "    'Sub-Categoria',\n",
    "    'Tipo',\n",
    "    'Seccion',\n",
    "    'Unidad',\n",
    "    'Frecuencia',\n",
    "    'Ajuste',\n",
    "    'Fuente',\n",
    "    'Pais',\n",
    "    'Departamento',\n",
    "    'Codigo_ISO',\n",
    "    'Categoria',\n",
    "    'Codigo_Seccion',\n",
    "    'Codigo_Arancel',\n",
    "    'Codigo_Capitulo',\n",
    "    'Codigo_Partida',\n",
    "    'Capitulo',         # vendrá de 'Desagregacion-2' si existía\n",
    "    'Arancel',          # vendrá de 'Desagregacion-4'\n",
    "    'Partida',          # vendrá de 'Desagregacion-3'\n",
    "    'Region',\n",
    "    'Pais_Origen',\n",
    "    'Via_Transporte',\n",
    "    'Regimen_Adicional',\n",
    "    'Colecturia_ID',\n",
    "    'Colecturia',\n",
    "    'Ciudad'\n",
    "]\n",
    "\n",
    "# Alinear el DataFrame a este orden (solo columnas que existan)\n",
    "cols_existentes = [c for c in ordered_columns if c in df_long.columns]\n",
    "df_long = df_long.reindex(columns=cols_existentes + [c for c in df_long.columns if c not in cols_existentes])\n",
    "\n",
    "# =============================================================================\n",
    "# 9. RESULTADO FINAL\n",
    "# =============================================================================\n",
    "print(df_long.head(10))\n",
    "print(\"Columnas finales:\", df_long.columns.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas nuevas en orden\n",
    "ordered_columns = [\n",
    "    'Fecha',\n",
    "    'Valor',\n",
    "    'Sub-Categoria',\n",
    "    'Tipo',\n",
    "    'Seccion',\n",
    "    'Unidad',\n",
    "    'Frecuencia',\n",
    "    'Ajuste',\n",
    "    'Fuente',\n",
    "    'Pais',\n",
    "    'Departamento',\n",
    "    'Codigo_ISO',\n",
    "    'Categoria',\n",
    "    'Codigo_Seccion',\n",
    "    'Codigo_Arancel',\n",
    "    'Codigo_Capitulo',\n",
    "    'Codigo_Partida',\n",
    "    'Capitulo',  # Vendrá de 'Desc_Capitulo' (si existía)\n",
    "    'Arancel',   # Vendrá de 'Desc_Arancel'\n",
    "    'Partida',   # Vendrá de 'Desc_Partida'\n",
    "    'Region',\n",
    "    'Pais_Origen',\n",
    "    'Via_Transporte',\n",
    "    'Regimen_Aduanero',  # Vendrá de 'Regimen_Adicional'\n",
    "    'Colecturia_ID',\n",
    "    'Colecturia',\n",
    "]\n",
    "\n",
    "# Mapeo para renombrar columnas\n",
    "column_mapping = {\n",
    "    'Desc_Seccion':'Seccion',\n",
    "    'Desc_Capitulo': 'Capitulo',\n",
    "    'Desc_Arancel': 'Arancel',\n",
    "    'Desc_Partida': 'Partida',\n",
    "    'Regimen_Adicional': 'Regimen_Aduanero'\n",
    "}\n",
    "\n",
    "# Renombrar las columnas del DataFrame según el mapeo\n",
    "df_long.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Reordenar columnas según la lista 'ordered_columns'\n",
    "df_long = df_long[ordered_columns]\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "print(df_long.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filas con al menos un valor NaN o None\n",
    "filas_nan = df_long[df_long.isnull().any(axis=1)]\n",
    "\n",
    "# Mostrar las filas encontradas\n",
    "print(filas_nan)\n",
    "df_long.dropna(inplace=True)\n",
    "filas_nan = df_long[df_long.isnull().any(axis=1)]\n",
    "print(filas_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supongdescribe(columns={mos que tu DataFrame se llama df y la columna que deseas ver es 'nombre_columna'\n",
    "\n",
    "# Mostrar la columna como una Serie de pandas\n",
    "print(df_long['Valor'])\n",
    "\n",
    "# Si prefieres verla como un DataFrame de una sola columna\n",
    "print(df_long[['Valor']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long.drop(['Mes', 'Ciudad','Columna_desagregacion'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Conexión a un archivo local DuckDB (se crea si no existe)\n",
    "con = duckdb.connect(\"mi_bd_local.duckdb\")\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# EJEMPLO DE DataFrame con columna \"Sub-Categoria\" y otras\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# 2) Registrar df_long en DuckDB => Tabla big_table\n",
    "con.register(\"df_long\", df_long)\n",
    "con.execute(\"CREATE OR REPLACE TABLE big_table AS SELECT * FROM df_long\")\n",
    "print(\"Tabla big_table creada en DuckDB.\")\n",
    "\n",
    "# Definir columnas de dimensión (todas menos 'Fecha' y 'Valor')\n",
    "dimension_cols = [c for c in df_long.columns if c not in ['Fecha', 'Valor']]\n",
    "\n",
    "# Si alguna columna tiene espacios o guiones, ponerla entre comillas para SQL\n",
    "dimension_cols_quoted = [f'\"{col}\"' for col in dimension_cols]\n",
    "cols_str = \", \".join(dimension_cols_quoted)\n",
    "\n",
    "# 3) Crear dimension_table con las columnas de dimensión (DISTINCT)\n",
    "create_dim_sql = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE dimension_table AS\n",
    "    SELECT DISTINCT {cols_str}\n",
    "    FROM big_table\n",
    "\"\"\"\n",
    "con.execute(create_dim_sql)\n",
    "print(\"Tabla dimension_table creada con las columnas de dimensión (DISTINCT).\")\n",
    "\n",
    "# 4) Generar dimension_id con row_number() sin usar UPDATE\n",
    "#    Creamos una tabla nueva (dimension_table_new) y luego reemplazamos.\n",
    "assign_id_sql = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE dimension_table_new AS\n",
    "    SELECT\n",
    "        row_number() OVER (ORDER BY {cols_str}) AS dimension_id,\n",
    "        d.*\n",
    "    FROM dimension_table d\n",
    "\"\"\"\n",
    "con.execute(assign_id_sql)\n",
    "\n",
    "con.execute(\"DROP TABLE dimension_table\")\n",
    "con.execute(\"ALTER TABLE dimension_table_new RENAME TO dimension_table\")\n",
    "print(\"Agregada columna dimension_id usando row_number().\")\n",
    "\n",
    "# 5) Crear la fact_table con JOIN: dimension_id, Fecha, Valor\n",
    "on_clause = \" AND \".join([f'b.\"{col}\" = d.\"{col}\"' for col in dimension_cols])\n",
    "create_fact_sql = f\"\"\"\n",
    "    CREATE OR REPLACE TABLE fact_table AS\n",
    "    SELECT \n",
    "        d.dimension_id,\n",
    "        b.\"Fecha\" AS Fecha,\n",
    "        b.\"Valor\" AS Valor\n",
    "    FROM big_table b\n",
    "    JOIN dimension_table d\n",
    "      ON {on_clause}\n",
    "\"\"\"\n",
    "con.execute(create_fact_sql)\n",
    "print(\"Tabla fact_table creada (JOIN).\")\n",
    "\n",
    "# 6) (Opcional) Verificar resultados\n",
    "dim_count = con.execute(\"SELECT COUNT(*) FROM dimension_table\").fetchone()[0]\n",
    "fact_count = con.execute(\"SELECT COUNT(*) FROM fact_table\").fetchone()[0]\n",
    "print(\"dimension_table registros:\", dim_count)\n",
    "print(\"fact_table registros:\", fact_count)\n",
    "\n",
    "# 7) (Opcional) Exportar a parquet\n",
    "con.execute(\"\"\"COPY dimension_table TO 'dimension_table.parquet' (FORMAT 'parquet')\"\"\")\n",
    "con.execute(\"\"\"COPY fact_table TO 'fact_table.parquet' (FORMAT 'parquet')\"\"\")\n",
    "print(\"Exportadas dimension_table.parquet y fact_table.parquet.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "def get_secret():\n",
    "    secret_name = \"RepDom-DB\"\n",
    "    region_name = \"us-east-1\"\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(service_name='secretsmanager', region_name=region_name)\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "\n",
    "    secret = json.loads(get_secret_value_response['SecretString'])\n",
    "    return secret\n",
    "\n",
    "secretos = get_secret()\n",
    "url_supabase: str = secretos['supabase_URL_RD']\n",
    "key: str = secretos['supabase_key_RD']\n",
    "\n",
    "# Instala la librería supabase-python si no la tienes: pip install supabase\n",
    "from supabase import create_client, Client\n",
    "\n",
    "supabase: Client = create_client(url_supabase, key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_rows_with_dimension_id(con, table_name: str, dim_id: int, limit=5):\n",
    "    \"\"\"\n",
    "    Muestra hasta 'limit' filas de 'table_name' donde dimension_id = dim_id.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {table_name}\n",
    "        WHERE dimension_id = {dim_id}\n",
    "        LIMIT {limit}\n",
    "    \"\"\"\n",
    "    df = con.execute(query).fetchdf()\n",
    "    print(f\"===== HEAD de la tabla {table_name} con dimension_id = {dim_id} (máx {limit} filas) =====\")\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo\n",
    "# Muestra 5 filas de 'fact_table' que tengan dimension_id = 100\n",
    "show_rows_with_dimension_id(con, 'fact_table', 1, 5)\n",
    "\n",
    "# Muestra 5 filas de 'dimension_table' con dimension_id = 100\n",
    "show_rows_with_dimension_id(con, 'dimension_table', 1, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "from supabase import create_client, Client\n",
    "\n",
    "def get_secret():\n",
    "    secret_name = \"RepDom-DB\"\n",
    "    region_name = \"us-east-1\"\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(service_name='secretsmanager', region_name=region_name)\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "\n",
    "    secret = json.loads(get_secret_value_response['SecretString'])\n",
    "    return secret\n",
    "\n",
    "\n",
    "def upload_same_dimension_id(\n",
    "    con: duckdb.DuckDBPyConnection,\n",
    "    supabase_client: Client,\n",
    "    dimension_table_name: str = \"dimension_table\",\n",
    "    fact_table_name: str = \"fact_table\",\n",
    "    dimension_target: str = \"tabladimension\",\n",
    "    fact_target: str = \"datos_light\",\n",
    "    same_dimension_id: int = 1\n",
    "):\n",
    "    \"\"\"\n",
    "    Este método:\n",
    "      1) Selecciona todas las filas de `dimension_table` en DuckDB donde dimension_id = same_dimension_id\n",
    "      2) Selecciona todas las filas de `fact_table` donde dimension_id = same_dimension_id\n",
    "      3) Renombra 'dimension_id' -> 'id' en ambos DataFrames (asumiendo que\n",
    "         en las tablas de Supabase la columna de PK/FK se llama 'id').\n",
    "      4) Convierte la columna 'Fecha' en la fact_table a string (YYYY-MM-DD), \n",
    "         para evitar error de JSON serializable.\n",
    "      5) Hace upsert de todas las filas de la dimensión en `dimension_target`.\n",
    "      6) Hace upsert de todas las filas de la fact table en `fact_target`.\n",
    "    \n",
    "    De esta forma, en Supabase tendrás:\n",
    "      - `tabladimension` con 'id' = same_dimension_id\n",
    "      - `datos_light` con 'id' = same_dimension_id\n",
    "    y así las dos tablas comparten el mismo ID.\n",
    "    \n",
    "    Ajusta según tu modelo real:\n",
    "      - Si en `datos_light` la columna se llama 'dimension_id', \n",
    "        no renombres y crea esa columna en Supabase.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Leer las filas de dimension_table con dimension_id = same_dimension_id\n",
    "    query_dim = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {dimension_table_name}\n",
    "        WHERE dimension_id = {same_dimension_id}\n",
    "    \"\"\"\n",
    "    df_dim = con.execute(query_dim).fetchdf()\n",
    "    if df_dim.empty:\n",
    "        print(f\"No se encontraron filas en {dimension_table_name} con dimension_id={same_dimension_id}\")\n",
    "    else:\n",
    "        print(f\"Filas de {dimension_table_name} con dimension_id={same_dimension_id}: {len(df_dim)}\")\n",
    "        # Renombrar 'dimension_id' -> 'id'\n",
    "        if 'dimension_id' in df_dim.columns:\n",
    "            df_dim.rename(columns={'dimension_id': 'id'}, inplace=True)\n",
    "\n",
    "    # 2) Leer las filas de fact_table con dimension_id = same_dimension_id\n",
    "    query_fact = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {fact_table_name}\n",
    "        WHERE dimension_id = {same_dimension_id}\n",
    "    \"\"\"\n",
    "    df_fact = con.execute(query_fact).fetchdf()\n",
    "    if df_fact.empty:\n",
    "        print(f\"No se encontraron filas en {fact_table_name} con dimension_id={same_dimension_id}\")\n",
    "    else:\n",
    "        print(f\"Filas de {fact_table_name} con dimension_id={same_dimension_id}: {len(df_fact)}\")\n",
    "\n",
    "        # Convertir la columna 'Fecha' a string para JSON\n",
    "        if 'Fecha' in df_fact.columns and pd.api.types.is_datetime64_any_dtype(df_fact['Fecha']):\n",
    "            df_fact['Fecha'] = df_fact['Fecha'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "        # Renombrar 'dimension_id' -> 'id'\n",
    "        if 'dimension_id' in df_fact.columns:\n",
    "            df_fact.rename(columns={'dimension_id': 'id'}, inplace=True)\n",
    "\n",
    "    # 3) Subir la dimensión\n",
    "    if not df_dim.empty:\n",
    "        dimension_dicts = df_dim.to_dict(orient='records')\n",
    "        try:\n",
    "            response_dim = supabase_client.table(dimension_target).upsert(dimension_dicts).execute()\n",
    "            print(f\"Upsert dimensión completado. Respuesta:\\n{response_dim}\")\n",
    "        except Exception as e:\n",
    "            print(\"Error al subir dimensión a Supabase:\", e)\n",
    "\n",
    "    # 4) Subir la fact table\n",
    "    if not df_fact.empty:\n",
    "        fact_dicts = df_fact.to_dict(orient='records')\n",
    "        try:\n",
    "            response_fact = supabase_client.table(fact_target).upsert(fact_dicts).execute()\n",
    "            print(f\"Upsert fact completado. Respuesta:\\n{response_fact}\")\n",
    "        except Exception as e:\n",
    "            print(\"Error al subir fact a Supabase:\", e)\n",
    "\n",
    "\n",
    "# Ejemplo de uso principal\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Obtener credenciales\n",
    "    secretos = get_secret()\n",
    "    url_supabase = secretos['supabase_URL_RD']\n",
    "    key = secretos['supabase_key_RD']\n",
    "\n",
    "    # 2) Crear cliente Supabase\n",
    "    supabase_client = create_client(url_supabase, key)\n",
    "\n",
    "    # 3) Conexión DuckDB\n",
    "    con = duckdb.connect(\"mi_bd_local.duckdb\")\n",
    "\n",
    "    # 4) Subir TODAS las filas que tengan dimension_id = 1\n",
    "    #    (o el dimension_id que necesites) a Supabase\n",
    "    upload_same_dimension_id(\n",
    "        con=con,\n",
    "        supabase_client=supabase_client,\n",
    "        dimension_table_name=\"dimension_table\",\n",
    "        fact_table_name=\"fact_table\",\n",
    "        dimension_target=\"tabladimension\",\n",
    "        fact_target=\"datos_light\",\n",
    "        same_dimension_id=1  # Ajusta el ID que deseas\n",
    "    )\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
